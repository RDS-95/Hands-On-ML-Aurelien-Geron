{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b532030a",
   "metadata": {},
   "source": [
    "# Natural Language Processing Using RNNs and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f9fd7",
   "metadata": {},
   "source": [
    "#### Generating Shakespeare using a character RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baeb34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97037062",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = 'https://homl.info/shakespeare'\n",
    "filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce0c3c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now encode every character as an integer\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47459611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['First'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "776d81d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b658bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e67a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the full text so each character is represented by its ID\n",
    "\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e149ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data set needs to be done carefully, since it is sequential\n",
    "\n",
    "dataset_size = encoded.shape[0]\n",
    "train_size = dataset_size * 90//100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b228fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to use the data sets window method to convert the long string of characters to smaller readable pieces.\n",
    "\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1531c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the data set for training\n",
    "\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e500840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can shuffle the windows \n",
    "\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f44d3158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we only havee 39 distinct characters so lets encode them as one hot vectors\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a9cc3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfe3a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the data has been prepared and we can move on to creating the model.\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None,max_id],\n",
    "                    dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                    dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efb0bd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "31368/31368 [==============================] - 11527s 367ms/step - loss: 1.6202\n",
      "Epoch 2/20\n",
      "31368/31368 [==============================] - 2454s 78ms/step - loss: 1.5383\n",
      "Epoch 3/20\n",
      "31368/31368 [==============================] - 2406s 77ms/step - loss: 1.5175\n",
      "Epoch 4/20\n",
      "31368/31368 [==============================] - 2402s 77ms/step - loss: 1.5066\n",
      "Epoch 5/20\n",
      "31368/31368 [==============================] - 2413s 77ms/step - loss: 1.4988\n",
      "Epoch 6/20\n",
      "31368/31368 [==============================] - 2417s 77ms/step - loss: 1.4936\n",
      "Epoch 7/20\n",
      "31368/31368 [==============================] - 35558s 1s/step - loss: 1.4896\n",
      "Epoch 8/20\n",
      "31368/31368 [==============================] - 2494s 79ms/step - loss: 1.4863\n",
      "Epoch 9/20\n",
      "31368/31368 [==============================] - 2570s 82ms/step - loss: 1.4838\n",
      "Epoch 10/20\n",
      "31368/31368 [==============================] - 278355s 9s/step - loss: 1.4810\n",
      "Epoch 11/20\n",
      "31368/31368 [==============================] - 2622s 84ms/step - loss: 1.4789\n",
      "Epoch 12/20\n",
      "31368/31368 [==============================] - 2602s 83ms/step - loss: 1.4769\n",
      "Epoch 13/20\n",
      "31368/31368 [==============================] - 12322s 393ms/step - loss: 1.4759\n",
      "Epoch 14/20\n",
      "31368/31368 [==============================] - 47499s 2s/step - loss: 1.4746\n",
      "Epoch 15/20\n",
      "31368/31368 [==============================] - 3717s 118ms/step - loss: 1.4732\n",
      "Epoch 16/20\n",
      "31368/31368 [==============================] - 3814s 122ms/step - loss: 1.4724\n",
      "Epoch 17/20\n",
      "31368/31368 [==============================] - 2877s 92ms/step - loss: 1.4717\n",
      "Epoch 18/20\n",
      "31368/31368 [==============================] - 8191s 261ms/step - loss: 1.4707\n",
      "Epoch 19/20\n",
      "31368/31368 [==============================] - 2885s 92ms/step - loss: 1.4698\n",
      "Epoch 20/20\n",
      "31368/31368 [==============================] - 4763s 152ms/step - loss: 1.4693\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "history = model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0412ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model predicts the next character written by Shakespeare. Preprocessing is required to feed text.\n",
    "\n",
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25a41c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 882ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test it out with a simple sentence\n",
    "\n",
    "X_new = preprocess(['How are yo'])\n",
    "Y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fd22694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    Y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(Y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def complete_text(text, n_chars=100, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a747c2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "Who are you profumed \n",
      "we laysing more sisters to his reason.\n",
      "\n",
      "katharina:\n",
      "well i never kath rival him my wife, i\n"
     ]
    }
   ],
   "source": [
    "#with these functions we can now print out some text.\n",
    "\n",
    "print(complete_text('Who are you', temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "89fd2325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has been a demonstration of a stateless RNN. In a stateful RNN, the final state from processing one batch\n",
    "#is used in the initial state for the next training batch.\n",
    "#this only makes sense if each input sequnce in a batch starts where the last one left off.\n",
    "#We need sequential, non-overlapping input sequences.\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ecee0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Stateful RNN\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                    dropout=0.2, recurrent_dropout=0.2,\n",
    "                    batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                    dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f866005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a small callback to reset the states\n",
    "\n",
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d29862bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#commenting this out for now for time constraints\n",
    "\n",
    "#model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "#model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f95f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has been a look at character level models. We can now look at a word level model.\n",
    "#Using word level models is useful in Sentiment Analysis. IMDb data set is popular for sentiment analysis\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d39206e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba957142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#The reviews have already been preprocessed. Can decode an individual review like this:\n",
    "\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "    id_to_word[id_] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa758355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ba2cabba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to ~\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa747954f804ddcbe2dbfc307502da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abd97b1d5594fee82be78ea62c3f345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteZLCN4C\\imdb_reviews-train.tfrecord*...…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteZLCN4C\\imdb_reviews-test.tfrecord*...:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteZLCN4C\\imdb_reviews-unsupervised.tfrec…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to ~\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#We can load the original reviews as byte strings from tensorflow\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load('imdb_reviews', as_supervised=True, with_info=True)\n",
    "train_size = info.splits['train'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3770e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write our preprocessing function to only keep the first 300 characters \n",
    "#uses regular expressions to replace any <br /> tages with spaces\n",
    "#replaces any characters other than letters and quotes with spaces\n",
    "#pads all reviews with a padding token to ensure the same length\n",
    "\n",
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b'<pad>'), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "48cce0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct our vocab by running through the entire training set and counting the occurences of each word\n",
    "\n",
    "from collections import Counter\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets['train'].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4178f113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check out the three most common words\n",
    "\n",
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cf146eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only the 10000 most common\n",
    "\n",
    "vocab_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "591f048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad95d0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[    9,    12,    11, 10791]], dtype=int64)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b'this movie was faaaaantastic'.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "28435d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can create our training set using the preprocessing steps.\n",
    "\n",
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets['train'].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c28339f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                          input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eba8712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "782/782 [==============================] - 42s 54ms/step - loss: 0.0355 - accuracy: 0.9883\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 43s 55ms/step - loss: 0.0286 - accuracy: 0.9906\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 45s 57ms/step - loss: 0.0245 - accuracy: 0.9918\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 45s 57ms/step - loss: 0.0247 - accuracy: 0.9918\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 45s 57ms/step - loss: 0.0200 - accuracy: 0.9939\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 45s 57ms/step - loss: 0.0164 - accuracy: 0.9945\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 45s 57ms/step - loss: 0.0192 - accuracy: 0.9936\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 45s 58ms/step - loss: 0.0165 - accuracy: 0.9943\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 45s 58ms/step - loss: 0.0157 - accuracy: 0.9948\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 45s 58ms/step - loss: 0.0145 - accuracy: 0.9956\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 45s 58ms/step - loss: 0.0107 - accuracy: 0.9968\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 45s 57ms/step - loss: 0.0088 - accuracy: 0.9973\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 45s 58ms/step - loss: 0.0065 - accuracy: 0.9979\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 45s 57ms/step - loss: 0.0080 - accuracy: 0.9973\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 45s 58ms/step - loss: 0.0042 - accuracy: 0.9986\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0050 - accuracy: 0.9986\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0038 - accuracy: 0.9988\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0038 - accuracy: 0.9987\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0057 - accuracy: 0.9982\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0054 - accuracy: 0.9982\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0044 - accuracy: 0.9988\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0029 - accuracy: 0.9991\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0031 - accuracy: 0.9987\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0021 - accuracy: 0.9993\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 8.3405e-04 - accuracy: 0.9996\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0037 - accuracy: 0.9989\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0049 - accuracy: 0.9986\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0032 - accuracy: 0.9990\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 9.7277e-04 - accuracy: 0.9996\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 45s 57ms/step - loss: 0.0025 - accuracy: 0.9993\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0015 - accuracy: 0.9993\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 1.2289e-04 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0038 - accuracy: 0.9990\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0033 - accuracy: 0.9990\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0021 - accuracy: 0.9992\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0021 - accuracy: 0.9994\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 44s 57ms/step - loss: 2.8039e-05 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 9.8466e-06 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 6.4650e-06 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 4.3418e-06 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 2.9300e-06 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 48s 62ms/step - loss: 1.9758e-06 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 46s 58ms/step - loss: 1.3386e-06 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 46s 58ms/step - loss: 9.0829e-07 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 1832s 2s/step - loss: 6.1657e-07 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 46s 58ms/step - loss: 4.1864e-07 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 46s 58ms/step - loss: 2.8435e-07 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "511e1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model needs to know that the padding tokens should be ignored\n",
    "#can be done simply by setting mask_zero=True\n",
    "\n",
    "#The following model is the same as the previous one, except it is built with functional API and handles masking manually\n",
    "\n",
    "K = keras.backend\n",
    "inputs = keras.layers.Input(shape=[None])\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(z)\n",
    "model = keras.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "413d499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can reuse pretrained embeddings from tensorflow\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer('https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1',\n",
    "                  dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6dcb701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5527 - accuracy: 0.7212\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5164 - accuracy: 0.7468\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5111 - accuracy: 0.7497\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5074 - accuracy: 0.7510\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5040 - accuracy: 0.7536\n"
     ]
    }
   ],
   "source": [
    "#by default, this is not trainable, but we can set trainable=True to fine tune for our task\n",
    "\n",
    "#load the imdb data and train the model.\n",
    "\n",
    "datasets, info = tfds.load('imdb_reviews', as_supervised=True, with_info=True)\n",
    "train_size = info.splits['train'].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets['train'].batch(batch_size).prefetch(1)\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7656a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next section will focus on neural machine translation\n",
    "\n",
    "#This code is a basic encoder decoder model\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n",
    "                                                output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, initial_state=encoder_state,\n",
    "    sequence_length=sequence_lengths)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "                   outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cde751f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#durign NLP, it is often useful to implement a bidirectional recurrent layer\n",
    "\n",
    "#keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23457bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
