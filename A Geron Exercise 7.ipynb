{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e113fd2",
   "metadata": {},
   "source": [
    "# Ensemble Learning Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b316f0f0",
   "metadata": {},
   "source": [
    "#### Q1: If you have trained 5 different models on the same training data and they all achieve 95% precision, is there any chance you can combine these models to get better results? How?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c5455a",
   "metadata": {},
   "source": [
    "Could implement a voting classifier taking the results of each and voting for the most popular class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9b9e7",
   "metadata": {},
   "source": [
    "#### Q2: Difference between hard and soft voting classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3077f5a3",
   "metadata": {},
   "source": [
    "In hard voting, each classifier has a prediction and the majority class is used as the voting classifiers prediction. In soft voting, the probabilities from each classifier are used to create an average over all classifiers and the voting classifier uses this as its output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d810d245",
   "metadata": {},
   "source": [
    "#### Q3: Can we speed up the training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests or stacking ensembles?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c8d89",
   "metadata": {},
   "source": [
    "For a boosting ensemble, the training steps are sequential. Spreading across multiple servers will not speed up training. For stacking, the evaluation of the previous layer must complete before the next can begin. For the other ensembles, spreading across multiple servers will help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac333e",
   "metadata": {},
   "source": [
    "#### Q4: Benefit of OOB evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a73fa9",
   "metadata": {},
   "source": [
    "The predictor can be evaluated on the set of OOB instances without the need for a seperate set of validation data. This is because training the predictor doesn't require any of the OOB instances. ie our training set can be bigger and produce better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021bbc9",
   "metadata": {},
   "source": [
    "#### Q5: What makes extra trees more random than random forests? How does this help and is it slower or faster than regular random forests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aea7dd",
   "metadata": {},
   "source": [
    "A random forest tries to find the optimum split for each feature. Extra trees chooses a random threshold for each feature and attempts to find the best split after. This makes them faster than random forests since they don't have to find the optimum split. It can also help when a random forest is overfitting as the extra tress random features splits act as a regularization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66cfd9",
   "metadata": {},
   "source": [
    "#### Load MNIST, split into test, training and validation. Train a random forest, an extra-trees & an SVM. Combine these using hard or soft voting into a classifier that outperforms the original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cb51ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "172d3f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c454d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist['data']\n",
    "y = mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25b19c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = X[:60000]\n",
    "y_train_full = y[:60000]\n",
    "X_test = X[60000:]\n",
    "y_test = y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8996ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_full[:50000], y_train_full[:50000]\n",
    "X_valid, y_valid = X_train_full[50000:], y_train_full[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2acb7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier 0.9743\n",
      "RandomForestClassifier 0.9736\n",
      "LinearSVC 0.8662\n"
     ]
    }
   ],
   "source": [
    "ext_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = LinearSVC(max_iter=100, tol=20, random_state=42)\n",
    "\n",
    "for clf in (ext_clf, rnd_clf, svm_clf):\n",
    "    clf.fit(X_train, y_train),\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9344435d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier 0.9737\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('ex', ext_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = voting_clf.predict(X_valid)\n",
    "\n",
    "print(voting_clf.__class__.__name__, accuracy_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1b30320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier 0.9749\n"
     ]
    }
   ],
   "source": [
    "#Try removing LinearSVC, change voting to soft.\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('ex', ext_clf), ('rf', rnd_clf)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = voting_clf.predict(X_valid)\n",
    "\n",
    "print(voting_clf.__class__.__name__, accuracy_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aea0d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier 0.9722\n",
      "RandomForestClassifier 0.9705\n",
      "VotingClassifier 0.9729\n"
     ]
    }
   ],
   "source": [
    "#looking good. Check on the test set now to see which performs the best.\n",
    "\n",
    "for clf in (ext_clf, rnd_clf, voting_clf):\n",
    "    clf.fit(X_train_full, y_train_full),\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f9712",
   "metadata": {},
   "source": [
    "#### Create a stacking ensemble using the classifiers from the previous question. First train a blender using a new training set created with the results of predictions on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b54d5c38",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m X_valid_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;28mlen\u001b[39m(X_valid), \u001b[38;5;28mlen\u001b[39m(estimators)), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(estimators):\n\u001b[1;32m----> 6\u001b[0m     X_valid_pred[:, index] \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(X_valid)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "estimators=[('ex', ext_clf), ('rf', rnd_clf)]\n",
    "\n",
    "X_valid_pred = np.empty((len(X_valid), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_valid_pred[:, index] = estimator.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33307f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
